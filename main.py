from subprocess import Popen, PIPE
import argparse
import statistics
import re
import json
import glob
import sys
from analysis import multi_linear_regression
from yaml import load
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper

from engine import InvokeEngine, EngineOutput

# TODO express this as configuration parameter instead of a constant
NUM_SMOOTHING_RUNS = 20

engines = [
    'wasmi'
]

def run_benchmark(engine, benchmark, wasm_input):
    if engine == 'wasmi':
        return EngineOutput.wasmi(InvokeEngine.wasmi(benchmark['wasm'], wasm_input))

def run_benchmarks(count_opcodes=False):
    engines = ['wasmi']
    datasets = {}

    for test_file in glob.iglob("./benchmarks/*.yml"):
        t = None
        with open(test_file, 'r') as f:
           t =  load(f, Loader=Loader)

        test_name = list(t.keys())[0]
        datasets[test_name] = { 'engines': {} }

        if count_opcodes:
            datasets[test_name]['opcode_counts'] = []

        test_dataset = datasets[test_name]

        test_case = t[test_name]

        for inp in test_case['inputs']:
            if count_opcodes:
                opcode_counts = EngineOutput.wasmi_instruction_counter(InvokeEngine.wasmi_instrumented(test_case['wasm'], inp))
                datasets[test_name]['opcode_counts'].append(opcode_counts)

            for engine in engines:
                execution_times = []

                if not engine in test_dataset['engines']:
                    test_dataset['engines'][engine] = [] 

                for i in range(0, NUM_SMOOTHING_RUNS):
                    execution_times.append(run_benchmark(engine, test_case, inp))

                test_dataset['engines'][engine].append(statistics.mean(execution_times))

    return datasets

def mlr_analysis(dataset):
    # TODO do MLR analysis for each engine separately
    for test_case in dataset:
        if not "opcode_counts" in dataset[test_case]:
            print("dataset must be prepared with opcode counting enabled for regression analysis...")
            return

        for engine in dataset[test_case]['engines']:
            xs = dataset[test_case]["opcode_counts"]
            ys = dataset[test_case]['engines'][engine]
            multi_linear_regression(ys, xs)

def main():
    parser = argparse.ArgumentParser(description='benchmark wasm using various engines.  Perform analysis on the results')
    subparsers = parser.add_subparsers(help = "Commands are benchmark, analyse", dest="cmd")
    subparsers.required = True

    parser_benchmark = subparsers.add_parser('benchmark', help = "benchmark wasm engines")
    parser_benchmark.add_argument('-c', '--count-opcodes', help="for each execution trace, get a list of the number of the occurances of each wasm opcode that was executed", action="store_true")

    parser_analyse = subparsers.add_parser("analyse", help = "do analysis on a dataset gathered by benchmark")
    parser_analyse.add_argument("-f", "--file", help="benchmarking json file generated by the 'benchmark' command", nargs=1)

    args = parser.parse_args()

    dataset = None

    if args.cmd == 'benchmark':
        print("gathering benchmarks...")
        dataset = run_benchmarks(count_opcodes = args.count_opcodes)
        with open('output/dataset.json', 'w') as f:
            json.dump(dataset, f)
    elif args.cmd == 'analyse':
        if args.file == None:
            print("--file option required for 'analyse' command")
            sys.exit(-1)

        dataset_file = args.file[0]

        with open(dataset_file, 'r') as f:
            dataset = json.load(f)

        mlr_analysis(dataset)

if __name__ == "__main__":
    main()

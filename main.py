from subprocess import Popen, PIPE
import argparse
import statistics
import re
import json
import glob
import sys
from analysis import multi_linear_regression
from yaml import load
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper

from engine import InvokeEngine, EngineOutput

# TODO express this as configuration parameter instead of a constant
NUM_SMOOTHING_RUNS = 1 # 10

engines = [
    'wasmi'
]

def run_benchmark(engine, benchmark, wasm_input):
    if engine == 'wasmi':
        return EngineOutput.wasmi(InvokeEngine.wasmi(benchmark['wasm'], wasm_input))

def run_benchmarks(count_opcodes=False):
    engines = ['wasmi']
    datasets = {}

    for test_file in glob.iglob("./benchmarks/*.yml"):
        t = None
        with open(test_file, 'r') as f:
           t =  load(f, Loader=Loader)

        datasets[list(t.keys())[0]] = { }

        if count_opcodes:
            datasets[list(t.keys())[0]]['opcode_counts'] = []

        test_dataset = datasets[list(t.keys())[0]]
        test_case = t[list(t.keys())[0]]

        for inp in test_case['inputs']:
            if count_opcodes:
                opcode_counts = EngineOutput.wasmi_instruction_counter(InvokeEngine.wasmi_instrumented(test_case['wasm'], inp))
                test_dataset['opcode_counts'].append(opcode_counts)

            for engine in engines:
                execution_times = []

                if not engine in test_dataset:
                    test_dataset[engine] = [] 

                for i in range(0, NUM_SMOOTHING_RUNS):
                    execution_times.append(run_benchmark(engine, test_case, inp))

                test_dataset[engine].append(statistics.mean(execution_times))

    return datasets

def main():
    parser = argparse.ArgumentParser(description='benchmark wasm using various engines.  Perform analysis on the results')
    subparsers = parser.add_subparsers(help = "Commands are benchmark, analyse", dest="cmd")
    subparsers.required = True

    parser_benchmark = subparsers.add_parser('benchmark', help = "benchmark wasm engines")
    parser_benchmark.add_argument('-c', '--count-opcodes', help="for each execution trace, get a list of the number of the occurances of each wasm opcode that was executed", action="store_true")

    parser_analyse = subparsers.add_parser("analyse", help = "do analysis on a dataset gathered by benchmark")
    parser_analyse.add_argument("-f", "--file", help="benchmarking json file generated by the 'benchmark' command", nargs=1)

    args = parser.parse_args()

    dataset = None

    if args.cmd == 'benchmark':
        print("gathering benchmarks...")
        dataset = run_benchmarks(count_opcodes = args.count_opcodes)
        with open('output/dataset.json', 'w') as f:
            json.dump(dataset, f)
    elif args.cmd == 'analyse':
        # TODO do mlr here
        dataset_file = args.file
        if dataset_file == None:
            raise Exception("--file option required for 'analyse' command")

        with open('output/dataset.json', 'r') as f:
            dataset = json.load(f)

        import pdb; pdb.set_trace()
        print("ran analysis")

if __name__ == "__main__":
    main()
